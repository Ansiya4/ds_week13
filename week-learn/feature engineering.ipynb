{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4426f7d-e191-4a32-9bbd-51c88e97399e",
   "metadata": {},
   "source": [
    "## 1.One Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f4cf19-9ac2-4d64-91a4-b82daaa7e5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f751a37-ef1e-4ab6-9af3-cce317f7ee0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data\n",
    "text_data = [\n",
    "    \"The quick brown fox\",\n",
    "    \"Jumped over the lazy dog\",\n",
    "    \"it is very difficult\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98bbd8d1-c43d-4c02-a1dc-ccdd70f44bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eab0b359-051f-4c91-ad46-1af50de51575",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(sentence) for sentence in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b40fda56-f6ab-4404-9e02-e2ea44eb2484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'quick', 'brown', 'fox'],\n",
       " ['Jumped', 'over', 'the', 'lazy', 'dog'],\n",
       " ['it', 'is', 'very', 'difficult']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "138a286f-9dc1-4dce-845f-c80d25a31eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox']\n",
      "['Jumped', 'over', 'the', 'lazy', 'dog']\n",
      "['it', 'is', 'very', 'difficult']\n"
     ]
    }
   ],
   "source": [
    "for sentence_tokens in tokenized_sentences:\n",
    "    print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1026a1ad-272a-4b97-8553-6856f6d3711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer to tokenize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "681f76ca-1916-4219-a936-f9a2624ae211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dense = X.toarray()\n",
    "X_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5eb6ba0f-f037-42d1-b6d0-02f235a4c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder()\n",
    "X_dense = X.toarray()\n",
    "# Fit and transform the data\n",
    "X_onehot = onehot_encoder.fit_transform(X_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b4247156-6662-4820-a29b-51e0087acfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded data:\n",
      "[[0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0.]\n",
      " [1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0.]\n",
      " [1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 1. 0. 1. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"One-hot encoded data:\")\n",
    "print(X_onehot.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51d33aa7-1c67-425c-80f4-d028c0a6ab4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['brown' 'difficult' 'dog' 'fox' 'is' 'it' 'jumped' 'lazy' 'over' 'quick'\n",
      " 'the' 'very']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\")\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875f2c4-4021-4bd9-84af-b04a6169c428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec43ee95-6772-400d-abd3-c75f3b86b4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded array:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "Vocabulary:\n",
      "['ansiya', 'shahir']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "data = ['ansiya', 'shahir']\n",
    "\n",
    "# Create a vocabulary (unique elements)\n",
    "vocab = sorted(set(data))\n",
    "\n",
    "# Create an instance of OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder()\n",
    "\n",
    "# Fit the encoder on the vocabulary\n",
    "onehot_encoder.fit(np.array(vocab).reshape(-1, 1))\n",
    "\n",
    "# Transform the data into one-hot encoded vectors\n",
    "encoded_data = onehot_encoder.transform(np.array(data).reshape(-1, 1))\n",
    "\n",
    "# Convert the sparse matrix to a dense array for visualization\n",
    "encoded_data_dense = encoded_data.toarray()\n",
    "\n",
    "# Print the one-hot encoded array\n",
    "print(\"One-hot encoded array:\")\n",
    "print(encoded_data_dense)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed35f25-b697-4543-99b0-c5044eb10ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ansiya, shahir],[hamad,muhammed]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "051e3f91-ebdb-465a-9fa6-9e785f7bd420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded array:\n",
      "[[[1. 0. 0. 0.]\n",
      "  [0. 0. 0. 1.]]\n",
      "\n",
      " [[0. 1. 0. 0.]\n",
      "  [0. 0. 1. 0.]]]\n",
      "Vocabulary:\n",
      "['ansiya', 'hamad', 'muhammed', 'shahir']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "data = [['ansiya', 'shahir'], ['hamad', 'muhammed']]\n",
    "\n",
    "# Flatten the nested list to get a list of all unique elements\n",
    "flat_data = [item for sublist in data for item in sublist]\n",
    "\n",
    "# Create a vocabulary (unique elements)\n",
    "vocab = sorted(set(flat_data))\n",
    "\n",
    "# Create an instance of OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder()\n",
    "\n",
    "# Fit the encoder on the vocabulary\n",
    "onehot_encoder.fit(np.array(vocab).reshape(-1, 1))\n",
    "\n",
    "# Transform the data into one-hot encoded vectors\n",
    "encoded_data = []\n",
    "\n",
    "for sublist in data:\n",
    "    # Transform each sublist into one-hot encoded vectors\n",
    "    encoded_sublist = onehot_encoder.transform(np.array(sublist).reshape(-1, 1))\n",
    "    # Convert the sparse matrix to a dense array and append it to the list\n",
    "    encoded_data.append(encoded_sublist.toarray())\n",
    "\n",
    "# Convert the list of arrays to a numpy array\n",
    "encoded_data_array = np.array(encoded_data)\n",
    "\n",
    "# Print the one-hot encoded array\n",
    "print(\"One-hot encoded array:\")\n",
    "print(encoded_data_array)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9714e-4358-433c-a81e-c1f91a9bede3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
